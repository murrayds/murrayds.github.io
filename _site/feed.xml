<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-01-29T20:31:45+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Dakota Murray</title><subtitle>personal description</subtitle><author><name>Dakota Murray</name><email>dakota.s.murray@gmail.com</email></author><entry><title type="html">Defending my dissertation</title><link href="http://localhost:4000/posts/2021/08/final-dissertation/" rel="alternate" type="text/html" title="Defending my dissertation" /><published>2021-08-26T00:00:00+00:00</published><updated>2021-08-26T00:00:00+00:00</updated><id>http://localhost:4000/posts/2021/08/final-dissertation</id><content type="html" xml:base="http://localhost:4000/posts/2021/08/final-dissertation/">&lt;p&gt;Well, I did it! I wrote my dissertation. I’m Dr. Murray now. A year of pre-k. 12 years of grade school. 5 years for my undergraduate degree. 5 more for this PhD. 23 years of my life have been spent in school! It’s a strange, scary, and exhilarating feeling to have finally reached the zenith of this long experience, and to stare into the distance ahead, wondering what’s next. The next steps, however, will have to wait. First, I want to catch my breath and look back at this latest achievement.&lt;/p&gt;

&lt;p&gt;I titled my dissertation &lt;em&gt;Embracing Complexity in the Science of Science&lt;/em&gt;.
Its a “stapled” dissertation format, merging 4 studies that I wrote while in grad school. The narrative thread I chose to connect each study was to look at science through the perspective of Complexity Science, considering how treating science as a complex system might inform us about its inner workings. You can &lt;a href=&quot;papers/murray_dissertation_final.pdf&quot;&gt;read it here&lt;/a&gt;, if you want (its quite long). If you’re more of a visual person, you might also appreciate &lt;a href=&quot;slides/dissertation_presentation&quot;&gt;this presentation&lt;/a&gt;, which I think did a pretty good job of summarizing it.&lt;/p&gt;

&lt;p&gt;The first of the four studies investigates the extent of gender and national bias in peer review, observing the role of &lt;em&gt;homophilic preferences&lt;/em&gt; in not only shaping review decisions, but structuring the composition of teams of referees.
The second study leverages public and private data to explore the factors driving student-teacher evaluations, an important but often overlooked metric in a scientist’s career; we find evidence of &lt;em&gt;demographic bias&lt;/em&gt;, namely that women and non-White faculty are at a disadvantage in these ratings, putting them at a disadvantage in hiring, promotion, and more. The third study attempts to quantify the extent of disagreement across science, revealing that it is most common in the social sciences and humanities, and lowest in physics and maths; yet there is also a great deal of heterogeneity across sub-fields, resulting from their the &lt;em&gt;physical realities&lt;/em&gt; of their research topic. Finally, the fourth study explores the examines the global landscape of scientific mobility, creating a dense representation of mobility that illustrates the importance of different kinds of &lt;em&gt;proximities&lt;/em&gt;, such as geography, language, culture, shared history, and prestige, in structuring the mobility decisions of individual scientists.&lt;/p&gt;

&lt;p&gt;Together, each of these studies constitutes a significant contribution to the Science of Science. Yet when viewed collectively through the perspective of complex systems, they show insights about the organization and behavior of science. For example, &lt;em&gt;homophilic preferences&lt;/em&gt;, &lt;em&gt;demographic bias&lt;/em&gt;, &lt;em&gt;physical realities&lt;/em&gt;, and &lt;em&gt;proximity&lt;/em&gt; are all examples of &lt;em&gt;bottom-up forces&lt;/em&gt; that govern the actions and interactions of individuals, allowing them to self-organize into complex social arrangements such as teams, communities, and disciplines. Concepts like &lt;em&gt;feedback&lt;/em&gt;, too, explain how selection mechanisms, like peer review (eLife), metric evaluations (student-teacher evaluations), peer judgements (disagreement), and hiring (mobility) naturally produce and perpetuate inequalities and hierarchies in science. Yet feedback also allows for radical change across science. The &lt;em&gt;chaotic&lt;/em&gt; nature of science sometimes triggers feedback loops that amplify the effects of certain events, such as an individual’s success or a breakthrough discovery, to such an extreme that they remake wide swathes of science over a few short years.&lt;/p&gt;

&lt;p&gt;By viewing science as a complex system, it shifts the attention of the Science of Science away from grand theories of science and universal laws, and instead towards the everyday actions and behaviors of individual scientists. By simply shifting our perspective, we can reveal something about the fundamental workings of science, and maybe even spot a path towards improving it, making it more open, equitable, and effective.&lt;/p&gt;

&lt;p&gt;There are of course tons of details left in my dissertation. Writing it took a long time, many revisions, and a lot of reading! Although I have wonderful mentors who praised my work, I remain intimately familiar with all of its shortcomings. Yet finally, I am beginning to look back and appreciate the work I have produced. It will never be perfect, but it was good enough to get me to where I want to go. And I’ve learned a lot from it too. I hope to take what I learned here and do even better research moving forward, because the dissertation shouldn’t be a barrier to your career, but a stepping stone.&lt;/p&gt;</content><author><name>Dakota Murray</name><email>dakota.s.murray@gmail.com</email></author><category term="dissertation" /><category term="career" /><category term="PhD" /><summary type="html">Well, I did it! I wrote my dissertation. I’m Dr. Murray now. A year of pre-k. 12 years of grade school. 5 years for my undergraduate degree. 5 more for this PhD. 23 years of my life have been spent in school! It’s a strange, scary, and exhilarating feeling to have finally reached the zenith of this long experience, and to stare into the distance ahead, wondering what’s next. The next steps, however, will have to wait. First, I want to catch my breath and look back at this latest achievement.</summary></entry><entry><title type="html">Graph Laplacian Eigenmaps</title><link href="http://localhost:4000/posts/2019/16/graph-laplacian-eigenmaps/" rel="alternate" type="text/html" title="Graph Laplacian Eigenmaps" /><published>2019-09-16T00:00:00+00:00</published><updated>2019-09-16T00:00:00+00:00</updated><id>http://localhost:4000/posts/2019/16/graph-laplacian-eigenmaps</id><content type="html" xml:base="http://localhost:4000/posts/2019/16/graph-laplacian-eigenmaps/">&lt;p&gt;Graph laplacian eigenmaps (GLE) are one means, among many, of embedding graphs into a low-dimensional numeric space. Specifically, GLE is an approach based on Matrix Factorization that takes as input non-relational data and outputs node embeddings. The key insight of GLEs is that the graph property to be preserved can be interpreted as pairwise node similarities. Thus, a larger penalty is imposed if two nodes with large similarity are embedded far apart. Thus, the goal of this approach is to find an embedding that minimizes this penalty.&lt;/p&gt;

&lt;p&gt;The gist of this approach is that it aims to first represent the graph as a Graph Laplacian Matrix, $L$. $L$ is a means of representing a graph in matrix form following the definition $L = D - W$, where $D$ is the &lt;em&gt;degree matrix&lt;/em&gt;, a diagonal matrix containing the degree of each node, and $W$ is the adjacency matrix of weights. Under this representation, all positive values correspond to degrees of the node, and all negative values to the weights of the edges. We then perform a decomposition on $L$ to find its eigenvalues. The eigenvectors corresponding to the smallest of the eigenvalues are used as the embedding.&lt;/p&gt;

&lt;p&gt;The optimal embedding, $y^{\ast}$ can be derived from the following objective function,&lt;/p&gt;

\[y^{\ast} = \text{argmin}\_{y} \sum\_{i\neq j} (y\_{i} - y\_{j}^{2} W\_{ij}) = \text{argmin}\_{y} y^{T}Ly\]

&lt;p&gt;Where $W_{ij}$ is the similarity matrix between every pair of nodes $v_{i}$ and $v_{j}$. $L$ is the graph laplacian matrix, $D$ is the diagonal matrix for which $D_{ii} = \sum_{i \neq j}W_{ij}$.&lt;/p&gt;

&lt;p&gt;The goal of this objective function is to find the embedding, $y^{\ast}$ that minimizes the error compared to the Graph Laplacian Matrix, $L$. One benefit of representing a graph with $L$ rather than a simple adjacency matrix is that the diagonal will contain the degrees of the graph; the larger the degree of a node, the more that its row will be “weighted” when computing the objective function. The practical effect is that larger degree nodes will have a greater impact on the embedding than lower-degree nodes.&lt;/p&gt;

&lt;p&gt;There are several expansions on this basic optimization, but in each of these variants, the optimal embedding $y^{\prime}$ are the eigenvectors of the laplacian matrix, $\lambda$, which can be calculated by solving the the eigenproblem $Wy = \lambda Dy$.&lt;/p&gt;

&lt;p&gt;Arrange the eigenvalues from smallest to largest. The first eigenvalue should be close to zero, and its corresponding eigenvector is not typically used for embedding. Rather, use the remaining eigenvalues and their corresponding eigenvectors to construct the $d$-dimensional embedding. For example, a 3-dimensional embedding can be constructed using the eigenvectors corresponding to the 2nd, 3rd, and 4th smallest eigenvalues. A 2-dimensional embedding can be constructed using the eigenvectors of the 2nd and 3rd smallest eigenvalues.&lt;/p&gt;

&lt;p&gt;Lets walk through this process in R.&lt;/p&gt;

&lt;p&gt;Consider the Karate network, shown below, which is a popular network exemplifying community structure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/post_images/graph_laplacian_eigenmaps/karate_graph.png&quot; alt=&quot;Example image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Having loaded this data into R, we can construct the Graph Laplacian Matrix, $L$, from the adjacency matrix $W$ and the degree matrix, $D$.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;W &amp;lt;- as.matrix(as.matrix.network(karate))
D &amp;lt;- diag(rowSums(W))
L &amp;lt;- D - W
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then, we can solve the eigenproblem $Wy = \lambda Dy$ and select the eigenvectors corresponding to the 2nd and 3rd smallest eigenvectors to produce an embedding.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;eig &amp;lt;- eigen(L, D)
col_dim &amp;lt;- dim(eig$vectors)[1]
vectors &amp;lt;- eig$vectors[,(col_dim - 1):(col_dim - 2)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The resulting embedding is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/post_images/graph_laplacian_eigenmaps/karate_embed.png&quot; alt=&quot;Example image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see that much of the structure from the original graph also appears in the embedding. As with the network, the embedding demonstrates a clear division between the two communities. Additionally, we see all the nodes that are directly connected to the opposing community are clustered together. Meanwhile, nodes for Actor 5, 6, 7, 11, and 17 are clustered apart form the other clusters, reflecting their somewhat isolated position in the original graph. Similarly, Actor 12 only maintains one connection to Mr. H, and is thus set far apart from the other nodes in the embedding.&lt;/p&gt;

&lt;p&gt;There are many uses for graph embeddings. They allow for fast computation of node distances and can serve as effective features for graph-based classification problems.&lt;/p&gt;

&lt;p&gt;This content was largely drawn from the following resources,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Cai, H., Zheng, V. W., &amp;amp; Chang, K. C.-C. (2017). A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications. ArXiv:1709.07604 [Cs]. Retrieved from http://arxiv.org/abs/1709.07604&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Belkin, M., &amp;amp; Niyogi, P. (2001). Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering. Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, 585–591. Retrieved from http://dl.acm.org/citation.cfm?id=2980539.2980616&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Dakota Murray</name><email>dakota.s.murray@gmail.com</email></author><category term="embedding" /><category term="networks" /><summary type="html">Graph laplacian eigenmaps (GLE) are one means, among many, of embedding graphs into a low-dimensional numeric space. Specifically, GLE is an approach based on Matrix Factorization that takes as input non-relational data and outputs node embeddings. The key insight of GLEs is that the graph property to be preserved can be interpreted as pairwise node similarities. Thus, a larger penalty is imposed if two nodes with large similarity are embedded far apart. Thus, the goal of this approach is to find an embedding that minimizes this penalty.</summary></entry></feed>